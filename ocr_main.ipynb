{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home2/pratyush/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images saved.\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"dataset_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "word_list = words.words()\n",
    "word_list = word_list[:1000] \n",
    "img_width, img_height = 256, 64\n",
    "\n",
    "\n",
    "font_path = \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\"  \n",
    "font_size = 32\n",
    "font = ImageFont.truetype(font_path, font_size)\n",
    "\n",
    "for idx, word in enumerate(word_list):\n",
    "    image = Image.new(\"RGB\", (img_width, img_height), \"white\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Get text bounding box and calculate width and height\n",
    "    bbox = draw.textbbox((0, 0), word, font=font)\n",
    "    text_width, text_height = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "    text_x = (img_width - text_width) // 2\n",
    "    text_y = (img_height - text_height) // 2\n",
    "\n",
    "    draw.text((text_x, text_y), word, font=font, fill=\"black\")\n",
    "\n",
    "    image.save(os.path.join(output_dir, f\"{word}_{idx}.png\"))\n",
    "\n",
    "    if idx % 1000 == 0:\n",
    "        print(f\"{idx} images saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class WordImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None, max_label_length=15):  # Set max length according to your needs\n",
    "        self.img_dir = img_dir\n",
    "        self.img_names = os.listdir(img_dir)\n",
    "        self.transform = transform\n",
    "        self.max_label_length = max_label_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_names[idx]\n",
    "        label = img_name.split('_')[0] \n",
    "        image = Image.open(os.path.join(self.img_dir, img_name)).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label_indices = [ord(c) for c in label]\n",
    "        label_indices = label_indices[:self.max_label_length] \n",
    "        padding_needed = self.max_label_length - len(label_indices)\n",
    "        label_indices += [0] * padding_needed \n",
    "        return image, torch.tensor(label_indices)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = WordImageDataset(output_dir, transform=transform, max_label_length=15)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CNN_RNN_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=256, num_layers=2):\n",
    "        super(CNN_RNN_Model, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # Input channels=3 for RGB\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "        \n",
    "        self.rnn_input_size = (img_width // 8) * 128  \n",
    "        self.lstm = nn.LSTM(self.rnn_input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        \n",
    "        batch_size, channels, height, width = x.size()\n",
    "        x = x.permute(0, 2, 1, 3).contiguous() \n",
    "        x = x.view(batch_size, height, -1)\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.0651701346588136\n",
      "Epoch 2, Loss: 1.2425566834831239\n",
      "Epoch 3, Loss: 1.057148558921814\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "vocab_size = 128 \n",
    "hidden_size = 256\n",
    "\n",
    "model = CNN_RNN_Model(vocab_size, hidden_size)\n",
    "\n",
    "# Check if CUDA is available and move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(3):  \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in data_loader:\n",
    "        # Move data and labels to GPU\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        batch_size, seq_len, _ = outputs.size()\n",
    "        \n",
    "        labels = labels[:, :seq_len]  \n",
    "        mask = labels != 0 \n",
    "        outputs = outputs[mask]  \n",
    "        labels = labels[mask]  \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(data_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Number of Correct Characters: 0.6782508304626806\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Metric Calculation\n",
    "model.eval()\n",
    "total_correct_chars = 0\n",
    "total_chars = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 2)\n",
    "        \n",
    "        batch_size, seq_len = labels.size()\n",
    "        labels = labels[:, :predicted.size(1)]  # Ensure labels and predicted have the same length\n",
    "        mask = labels != 0  # Apply the same mask to labels\n",
    "        predicted = predicted[mask]\n",
    "        labels = labels[mask]\n",
    "        \n",
    "        correct_chars = (predicted == labels).sum().item()\n",
    "        total_correct_chars += correct_chars\n",
    "        total_chars += labels.size(0)\n",
    "\n",
    "avg_correct_chars = total_correct_chars / total_chars\n",
    "print(f\"Epoch {epoch + 1}, Average Number of Correct Characters: {avg_correct_chars}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
